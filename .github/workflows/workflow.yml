name: Build and Test

on: [push]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.9", "3.10", "3.11"]
        exclude:
          - os: macos-latest
            python-version: "3.9"
          - os: macos-latest
            python-version: "3.10"

    steps:
      - name: Maximize build space
        if: startsWith(matrix.os,'ubuntu-')
        uses: easimon/maximize-build-space@master
        with:
          overprovision-lvm: true
          remove-dotnet: true
          remove-android: true
          remove-haskell: true
          remove-codeql: true
          remove-docker-images: true

      - uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - run: python -m pip install -U pip
      - run: python -m pip install -U -r requirements.txt
      - name: Install flake8, black, mypy and ruff
        run: |
          python -m pip install flake8
          python -m pip install black
          python -m pip install mypy
          python -m pip install ruff

      - name: Cache Model for CTranslate2Engine
        id: cache-ctranslate2-model
        uses: actions/cache@v3
        with:
          path: line-sft
          key: ctranslate2-model
          restore-keys: ctranslate2-model-${{ hashFiles('line-sft/config.json', 'line-sft/model.bin', 'line-sft/vocabulary.json') }}
          enableCrossOsArchive: true
      - name: Fetch Model for CTranslate2Engine
        if: steps.cache-ctranslate2-model.outputs.cache-hit != 'true'
        run: ct2-transformers-converter --model line-corporation/japanese-large-lm-3.6b-instruction-sft --low_cpu_mem_usage --output_dir line-sft --quantization int8 --force

      - name: Cache Model for LlamaCppEngine
        id: cache-llama_cpp-model
        uses: actions/cache@v3
        with:
          path: models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf
          key: llama_cpp-model
          restore-keys: llama_cpp-model-${{ hashFiles('models/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf') }}
          enableCrossOsArchive: true
      - name: Fetch Model for LlamaCppEngine
        if: steps.cache-llama_cpp-model.outputs.cache-hit != 'true'
        run: wget -P models https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/resolve/main/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf

      - name: Lint with flake8
        run: python -m flake8 --exclude .venv/,.cache --max-line-length=88 .
      - name: Lint with black
        run: python -m black --check --extend-exclude .venv --diff --line-length 88 --skip-string-normalization .
      - name: Lint with mypy
        run: python -m mypy .
      - name: Lint with ruff
        run: |
          python -m ruff --format=github .

      - name: Test with pytest
        run: |
          python -m pytest
